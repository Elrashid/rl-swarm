Map:   0%|          | 0/4 [00:00<?, ? examples/s]Map: 100%|##########| 4/4 [00:00<00:00, 355.54 examples/s]
Map:   0%|          | 0/4 [00:00<?, ? examples/s]Map: 100%|##########| 4/4 [00:00<00:00, 374.85 examples/s]
Map:   0%|          | 0/4 [00:00<?, ? examples/s]Map: 100%|##########| 4/4 [00:00<00:00, 304.04 examples/s]
Map:   0%|          | 0/4 [00:00<?, ? examples/s]Map: 100%|##########| 4/4 [00:00<00:00, 310.49 examples/s]
ERROR:rgym_exp.vendor.genrl.logging_utils.global_defs:Exception occurred during game run.
Traceback (most recent call last):
  File "/content/rl-swarm/rgym_exp/vendor/genrl/game/game_manager.py", line 169, in run_game
    self.run_game_round()  # Loops through stages until end of round signal is received
    ^^^^^^^^^^^^^^^^^^^^^
  File "/content/rl-swarm/rgym_exp/src/manager.py", line 141, in run_game_round
    self.trainer.train(self.state, self.data_manager, self.rewards)
  File "/content/rl-swarm/rgym_exp/vendor/genrl/trainer/grpo_trainer.py", line 380, in train
    global_step = self.step(
                  ^^^^^^^^^^
  File "/content/rl-swarm/rgym_exp/vendor/genrl/trainer/grpo_trainer.py", line 447, in step
    loss.backward()
  File "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py", line 647, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py", line 829, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacity of 22.16 GiB of which 1.40 GiB is free. Process 28164 has 726.00 MiB memory in use. Process 28937 has 2.00 GiB memory in use. Process 29109 has 9.94 GiB memory in use. Process 29746 has 6.04 GiB memory in use. Process 29844 has 2.06 GiB memory in use. Of the allocated memory 5.30 GiB is allocated by PyTorch, and 528.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Stack (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/content/rl-swarm/rgym_exp/runner/swarm_launcher.py", line 298, in <module>
    main()
  File "/content/rl-swarm/rgym_exp/runner/swarm_launcher.py", line 283, in main
    game_manager.run_game()
  File "/content/rl-swarm/rgym_exp/vendor/genrl/game/game_manager.py", line 171, in run_game
    get_logger().exception(
