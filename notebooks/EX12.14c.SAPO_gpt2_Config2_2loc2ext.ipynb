{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SAPO Config 2: 5 GPT-2 Nodes (I=4, J=4) on Single A100 80GB **BEST**\n\nThis notebook runs **SAPO Config 2** with **balanced swarm collaboration** (50% external rollouts).\n\n**Configuration:**\n- **I=4** (local rollouts per round)\n- **J=4** (external rollouts per round - 50% external)\n- **G=8** (completions per question)\n- **Model**: GPT-2 (124M params)\n- **Hardware**: 5 nodes (1 coordinator + 4 workers) on 1× A100 80GB\n\n**Purpose:** Test balanced swarm collaboration - **BEST CONFIG** per paper.\n\n**Expected Results:**\n- Cumulative reward: **500-700**\n- Improvement vs baseline: **+110-150%**\n- Paper (Qwen2.5-0.5B): 1093 (+94% vs their baseline)\n\n**Memory Usage:** ~26-28 GB peak VRAM (coordinator uses less memory, workers train)\n\n**Timeline:** ~21 hours (2000 rounds)\n\n**Scientific Justification:** See `EXPERIMENTAL_DESIGN_JUSTIFICATION.md`\n\n**Paper Reference:** arXiv:2509.08721 - SAPO (Gensyn AI Team, 2025)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configuration\n\n**This notebook is pre-configured for Config 2 (I=4, J=4) - BEST CONFIG.**\n\nJust run all cells - no changes needed!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SAPO Config 2 Experiment Configuration (I=4, J=4) - BEST CONFIG\n# This notebook runs 5 nodes with BALANCED swarm collaboration (50% external)\n\n# ============================================\n# PRE-CONFIGURED FOR CONFIG 2 (BEST)\n# ============================================\nEXPERIMENT_NAME = 'sapo_gpt2_config2_4loc4ext'\nNUM_TRAIN_SAMPLES = 4        # I: Local rollouts per round\nNUM_TRANSPLANT_TREES = 4     # J: External rollouts (50% external)\n\n# ============================================\n# FIXED SETTINGS (same for all experiments)\n# ============================================\nNUM_NODES = 5                # Run 5 nodes (1 coordinator + 4 workers)\nMODEL_NAME = 'gpt2'          # GPT-2 (124M params, fits memory)\nNUM_GENERATIONS = 8          # G: Completions per question (like paper)\nMAX_ROUNDS = 2000            # Train for 2000 rounds (like paper)\nSEED = 42                    # For reproducibility\n\n# Rollout Sharing Configuration\nROLLOUT_PUBLISH_FREQUENCY = 'stage'  # When to share rollouts\nROLLOUT_CLEANUP_ENABLED = True       # Enable cleanup to save space\nROLLOUT_KEEP_LAST_N_ROUNDS = 20      # Keep recent rollouts only\nROLLOUT_ARCHIVE_OLD = False          # Don't archive (saves space)\n\n# Checkpoint Configuration\nCHECKPOINT_INTERVAL = 0          # Save checkpoints every N rounds (0=disabled, 10=default)\nMAX_STAGES = 1                   # Stages per round (1=default, higher=more training per round)\n\n# Optional: HuggingFace Token\nHUGGINGFACE_TOKEN = None  # Set to your token or keep None\n\nprint(\"=\"*60)\nprint(f\"SAPO Config 2 Experiment (BEST)\")\nprint(\"=\"*60)\nprint(f\"✓ Nodes: {NUM_NODES} (1 coordinator + 4 workers on single A100 80GB)\")\nprint(f\"✓ Model: {MODEL_NAME}\")\nprint(f\"✓ Config: I={NUM_TRAIN_SAMPLES}, J={NUM_TRANSPLANT_TREES}, G={NUM_GENERATIONS}\")\nprint(f\"✓ Experiment: {EXPERIMENT_NAME}\")\nprint(f\"✓ Max Rounds: {MAX_ROUNDS}\")\nprint()\nprint(f\"Expected VRAM: ~26-28 GB peak (coordinator uses less memory)\")\nprint(f\"Expected Time: ~21 hours\")\nprint()\nprint(\"📊 Config 2 (balanced swarm collaboration - 50% external) **BEST**\")\nprint(\"   Expected reward: 500-700 (+110-150% vs baseline)\")\nprint(\"   Paper (Qwen2.5): 1093 (+94%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set base path (MUST BE SAME ACROSS ALL NODES)\n",
    "GDRIVE_BASE_PATH = '/content/drive/MyDrive/rl-swarm'\n",
    "os.makedirs(GDRIVE_BASE_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Google Drive mounted at: {GDRIVE_BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. System Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nprint(\"=\"*60)\nprint(\"GPU Verification\")\nprint(\"=\"*60)\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    print(f\"✓ GPU: {gpu_name}\")\n    print(f\"✓ Total VRAM: {total_memory:.1f} GB\")\n    print()\n    \n    # Check if we have enough memory\n    # Note: Coordinator uses ~6.5 GB but doesn't train (less memory for gradients/activations)\n    # Workers: 4 × 6.5 GB = 26 GB\n    # Coordinator: ~3-4 GB (model only, no training overhead)\n    required_memory = (NUM_NODES - 1) * 6.5 + 4  # 4 workers + coordinator\n    print(f\"Memory Requirements:\")\n    print(f\"  Workers: {NUM_NODES - 1} × 6.5 GB = {(NUM_NODES - 1) * 6.5:.1f} GB\")\n    print(f\"  Coordinator: ~4 GB (non-training)\")\n    print(f\"  Total estimated: {required_memory:.1f} GB\")\n    print(f\"  Available: {total_memory:.1f} GB\")\n    print(f\"  Margin: {total_memory - required_memory:.1f} GB\")\n    print()\n    \n    if total_memory < required_memory:\n        print(\"⚠️  WARNING: Insufficient VRAM!\")\n        print(f\"   Need at least {required_memory:.0f} GB, but have {total_memory:.1f} GB\")\n        print(f\"   Consider reducing NUM_NODES\")\n        raise RuntimeError(\"Insufficient GPU memory\")\n    elif total_memory < 75:\n        print(\"⚠️  WARNING: Tight fit! Expected A100 80GB.\")\n        print(f\"   Have {total_memory:.1f} GB. May still work, but monitor memory closely.\")\n    else:\n        print(f\"✅ Sufficient VRAM for {NUM_NODES} nodes ({NUM_NODES - 1} training + 1 coordinator)\")\nelse:\n    raise RuntimeError(\"No GPU detected! Select A100 GPU runtime: Runtime > Change runtime type > A100 GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%cd /content\n\n# Remove existing directory if it exists\nif os.path.exists('/content/rl-swarm'):\n    print(\"Removing existing repository...\")\n    !rm -rf /content/rl-swarm\n\n# Clone fresh copy\nprint(\"Cloning repository...\")\n!git clone https://github.com/Elrashid/rl-swarm.git /content/rl-swarm\n\n# Change to repo directory\n%cd /content/rl-swarm\n\n# Verify clone worked\nif not os.path.exists('requirements.txt'):\n    raise FileNotFoundError(\"Repository clone failed - requirements.txt not found\")\n\nprint(\"✓ Repository cloned successfully\")\nprint()\n\n# Install dependencies\nprint(\"Installing dependencies (this may take 3-5 minutes)...\")\nprint(\"Note: Warnings about protobuf versions can be ignored\")\nprint()\n\n# Install main dependencies (without -q to show errors)\n!pip install -r requirements.txt\n\n# Install GenRL explicitly\n!pip install gensyn-genrl==0.1.9\n\n# Fix protobuf version explicitly to avoid conflicts\n!pip install 'protobuf>=4.25.0,<5.0'\n\n# Verify reasoning-gym was installed\ntry:\n    import reasoning_gym\n    print()\n    print(\"✓ Dependencies installed successfully\")\n    print(\"✓ reasoning-gym verified\")\nexcept ImportError as e:\n    print()\n    print(\"❌ ERROR: reasoning-gym failed to install!\")\n    print(\"   Please report this issue with the error above\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Experiment\n",
    "\n",
    "**Note:** Only the coordinator (node_0) creates the experiment structure. Workers will join it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgym_exp.utils.experiment_manager import init_experiment\n",
    "\n",
    "# Initialize experiment structure in Google Drive (coordinator creates it)\n",
    "config_overrides = {\n",
    "    'training.max_round': MAX_ROUNDS,\n",
    "    'training.num_generations': NUM_GENERATIONS,\n",
    "    'training.num_transplant_trees': NUM_TRANSPLANT_TREES,\n",
    "    'training.num_train_samples': NUM_TRAIN_SAMPLES,\n",
    "    'training.seed': SEED,\n",
    "}\n",
    "\n",
    "init_experiment(\n",
    "    gdrive_base_path=GDRIVE_BASE_PATH,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    config_overrides=config_overrides\n",
    ")\n",
    "\n",
    "print(f\"✓ Experiment initialized: {EXPERIMENT_NAME}\")\n",
    "print(f\"  Path: {GDRIVE_BASE_PATH}/experiments/{EXPERIMENT_NAME}\")\n",
    "print(f\"  Config: I={NUM_TRAIN_SAMPLES}, J={NUM_TRANSPLANT_TREES}, G={NUM_GENERATIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Launch 5-Node Swarm (KEY CELL)\n\n**This cell:**\n1. Spawns 5 separate Python processes\n2. Each process runs `swarm_launcher.py` with unique NODE_ID\n3. All processes share GPU 0 (CUDA_VISIBLE_DEVICES=0)\n4. Coordinator (node_0) manages round progression\n5. Workers (node_1-4) follow coordinator\n\n**Logs:** Each node writes to Google Drive at `{GDRIVE_BASE_PATH}/experiments/{EXPERIMENT_NAME}/logs/`\n\n**Monitor:** Use next cell (Cell 7) to track progress in real-time"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport os\nimport time\nfrom datetime import datetime\n\nprint(\"=\"*60)\nprint(f\"Launching {NUM_NODES}-Node SAPO Swarm\")\nprint(\"=\"*60)\nprint(f\"Experiment: {EXPERIMENT_NAME}\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Config: I={NUM_TRAIN_SAMPLES}, J={NUM_TRANSPLANT_TREES}, G={NUM_GENERATIONS}\")\nprint(f\"Hardware: All {NUM_NODES} nodes on single GPU (A100 80GB)\")\nprint(\"=\"*60)\nprint()\n\n# =========================================\n# CODE VERSION CHECK\n# =========================================\nprint(\"Checking code version...\")\nresult = subprocess.run(['git', 'log', '--oneline', '-5'], \n                       capture_output=True, text=True, cwd='/content/rl-swarm')\ncommits = result.stdout.strip().split('\\n')\nlatest_commit = commits[0] if commits else \"unknown\"\n\nprint(f\"Latest commit: {latest_commit}\")\nprint(f\"Recent commits:\")\nfor commit in commits[:3]:\n    print(f\"  {commit}\")\nprint()\n\n# Check for critical fixes\nhas_round_fix = any('724106c' in commit or 'round mismatch' in commit.lower() \n                    for commit in commits)\nhas_docs = any('65d5780' in commit or 'CODE_EXECUTION_FLOW' in commit \n               for commit in commits)\n\nif not has_round_fix:\n    print(\"⚠️  WARNING: Code may not have round-1 fetch fix!\")\n    print(\"   This fix is CRITICAL for rollout sharing to work.\")\n    print(\"   Consider: !cd /content/rl-swarm && git pull origin main\")\n    print()\n    user_input = input(\"Continue anyway? (yes/no): \")\n    if user_input.lower() != 'yes':\n        raise RuntimeError(\"Aborted: Please pull latest code first\")\nelse:\n    print(\"✓ Has round-1 fetch fix (commit 724106c)\")\n\nprint()\nprint(\"=\"*60)\nprint(\"Starting node processes...\")\nprint(\"=\"*60)\nprint()\n\nprocesses = []\nprocess_stderr = []  # Store stderr for each process\nstart_time = time.time()  # For ETA calculation\n\nfor node_id in range(NUM_NODES):\n    # Environment variables for this node\n    env = os.environ.copy()\n    env['NODE_ID'] = f'node_{node_id}'\n    env['NODE_ROLE'] = 'coordinator' if node_id == 0 else 'worker'\n    env['MODEL_NAME'] = MODEL_NAME\n    env['NUM_TRAIN_SAMPLES'] = str(NUM_TRAIN_SAMPLES)\n    env['NUM_TRANSPLANT_TREES'] = str(NUM_TRANSPLANT_TREES)\n    env['NUM_GENERATIONS'] = str(NUM_GENERATIONS)\n    env['MAX_ROUNDS'] = str(MAX_ROUNDS)\n    env['EXPERIMENT_NAME'] = EXPERIMENT_NAME\n    env['GDRIVE_PATH'] = GDRIVE_BASE_PATH\n    env['CUDA_VISIBLE_DEVICES'] = '0'  # All nodes share GPU 0\n    env['SEED'] = str(SEED + node_id)  # Different seed per node (diversity)\n    env['ROLLOUT_PUBLISH_FREQUENCY'] = ROLLOUT_PUBLISH_FREQUENCY\n    env['ROLLOUT_CLEANUP_ENABLED'] = str(ROLLOUT_CLEANUP_ENABLED)\n    env['ROLLOUT_KEEP_LAST_N_ROUNDS'] = str(ROLLOUT_KEEP_LAST_N_ROUNDS)\n    env['ROLLOUT_ARCHIVE_OLD'] = str(ROLLOUT_ARCHIVE_OLD)\n    env['CHECKPOINT_INTERVAL'] = str(CHECKPOINT_INTERVAL)\n    env['MAX_STAGES'] = str(MAX_STAGES)\n    \n    if HUGGINGFACE_TOKEN:\n        env['HUGGINGFACE_ACCESS_TOKEN'] = HUGGINGFACE_TOKEN\n    \n    # Launch process with stderr/stdout capture\n    import sys\n    process = subprocess.Popen(\n        [sys.executable, '-m', 'rgym_exp.runner.swarm_launcher'],\n        env=env,\n        cwd='/content/rl-swarm',\n        stderr=subprocess.PIPE,  # Capture errors\n        stdout=subprocess.PIPE,  # Capture output\n        text=True\n    )\n    processes.append(process)\n    process_stderr.append(process.stderr)\n    \n    role = \"COORDINATOR\" if node_id == 0 else \"WORKER     \"\n    print(f\"✓ Started node_{node_id} ({role}) - PID: {process.pid:5d}\")\n    \n    # Stagger startup to avoid race conditions\n    time.sleep(10)\n\nprint()\nprint(\"=\"*60)\nprint(\"PROCESS HEALTH CHECK\")\nprint(\"=\"*60)\nprint(\"Waiting 30 seconds for processes to initialize...\")\ntime.sleep(30)\n\ncrashed_nodes = []\nfor i, p in enumerate(processes):\n    returncode = p.poll()\n    if returncode is not None:\n        crashed_nodes.append(i)\n        print(f\"❌ node_{i}: CRASHED (exit code {returncode})\")\n        # Try to read stderr\n        try:\n            stderr_output = process_stderr[i].read()\n            if stderr_output:\n                print(f\"   Error output:\")\n                for line in stderr_output.split('\\n')[-10:]:  # Last 10 lines\n                    if line.strip():\n                        print(f\"     {line}\")\n        except:\n            print(f\"   (Could not read error output)\")\n    else:\n        print(f\"✓ node_{i}: RUNNING\")\n\nprint()\n\nif crashed_nodes:\n    print(f\"⚠️  WARNING: {len(crashed_nodes)}/{NUM_NODES} nodes crashed!\")\n    print(f\"   Crashed: {', '.join(f'node_{i}' for i in crashed_nodes)}\")\n    print(f\"   Check logs in: {GDRIVE_BASE_PATH}/experiments/{EXPERIMENT_NAME}/logs/\")\n    print()\n    user_input = input(\"Continue with remaining nodes? (yes/no): \")\n    if user_input.lower() != 'yes':\n        print(\"Terminating all processes...\")\n        for p in processes:\n            if p.poll() is None:\n                p.terminate()\n        raise RuntimeError(f\"{len(crashed_nodes)} nodes crashed - see errors above\")\nelse:\n    print(f\"✅ All {NUM_NODES} nodes launched successfully!\")\n\nprint()\nprint(f\"✓ Training will run for approximately 21 hours ({MAX_ROUNDS} rounds)\")\nprint(f\"✓ Logs location: {GDRIVE_BASE_PATH}/experiments/{EXPERIMENT_NAME}/logs/\")\nprint()\nprint(\"⚠️  Keep this notebook open (browser tab active)\")\nprint(\"⚠️  Colab may disconnect after 12-24 hours\")\nprint(\"⚠️  Training will continue, but use Cell 7 to monitor\")\nprint()\nprint(\"Monitor progress in Cell 7 below...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Monitor Training Progress\n\n**This cell:**\n- Shows real-time status of all 5 nodes\n- Displays GPU memory usage\n- Shows current round/stage progress\n- Estimates time remaining (ETA)\n- Updates every 60 seconds\n\n**To stop training:** Click \"Stop\" button or press Ctrl+C\n\n**Note:** You can re-run this cell anytime to check status"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom IPython.display import clear_output\nimport pandas as pd\n\nprint(\"Starting training monitor...\")\nprint(\"Press 'Stop' button or Ctrl+C to interrupt\\n\")\n\nmonitor_start_time = time.time()\n\ntry:\n    while True:\n        clear_output(wait=True)\n        \n        # Check process status\n        running = sum(1 for p in processes if p.poll() is None)\n        completed = NUM_NODES - running\n        \n        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        elapsed_hours = (time.time() - start_time) / 3600\n        \n        print(\"=\"*70)\n        print(f\" SAPO Training Monitor - {EXPERIMENT_NAME}\")\n        print(f\" Time: {current_time} | Elapsed: {elapsed_hours:.1f}h\")\n        print(\"=\"*70)\n        print()\n        \n        # Node status\n        print(f\"Nodes:\")\n        print(f\"  Running:   {running}/{NUM_NODES}\")\n        print(f\"  Completed: {completed}/{NUM_NODES}\")\n        print()\n        \n        # GPU memory\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated(0) / 1e9\n            reserved = torch.cuda.memory_reserved(0) / 1e9\n            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n            \n            utilization = (reserved / total) * 100\n            \n            print(f\"GPU Memory ({torch.cuda.get_device_name(0)}):\")\n            print(f\"  Allocated: {allocated:5.1f} GB\")\n            print(f\"  Reserved:  {reserved:5.1f} GB / {total:.1f} GB ({utilization:.1f}%)\")\n            print(f\"  Free:      {total - reserved:5.1f} GB\")\n            \n            # Warning if memory is high\n            if utilization > 90:\n                print(f\"  ⚠️  WARNING: High memory usage! May OOM soon.\")\n            elif utilization > 75:\n                print(f\"  ⚠️  Memory usage elevated. Monitoring closely.\")\n            \n            print()\n        \n        # Training progress\n        try:\n            from rgym_exp.utils.experiment_manager import get_experiment_status\n            status = get_experiment_status(GDRIVE_BASE_PATH, EXPERIMENT_NAME)\n            \n            if status:\n                current_round = status.get('current_round', 0)\n                progress_pct = (current_round / MAX_ROUNDS) * 100\n                \n                print(f\"Training Progress:\")\n                print(f\"  Round:     {current_round:4d} / {MAX_ROUNDS} ({progress_pct:5.1f}%)\")\n                print(f\"  Stage:     {status.get('current_stage', 0)}\")\n                print(f\"  Active peers: {status.get('active_peers', 0)}\")\n                \n                # ETA calculation\n                if current_round > 10:  # Wait for stable estimate\n                    hours_per_round = elapsed_hours / current_round\n                    remaining_rounds = MAX_ROUNDS - current_round\n                    eta_hours = remaining_rounds * hours_per_round\n                    \n                    print(f\"  ETA:       {eta_hours:.1f} hours (~{eta_hours/24:.1f} days)\")\n                    \n                    # Progress bar\n                    bar_length = 40\n                    filled = int(bar_length * progress_pct / 100)\n                    bar = '█' * filled + '░' * (bar_length - filled)\n                    print(f\"  [{bar}]\")\n                \n                print()\n                \n                # Recent performance\n                try:\n                    from rgym_exp.utils.experiment_manager import get_experiment_metrics\n                    df = get_experiment_metrics(GDRIVE_BASE_PATH, EXPERIMENT_NAME)\n                    \n                    if not df.empty:\n                        cumulative_reward = df['my_reward'].sum()\n                        recent_reward = df.tail(10)['my_reward'].mean()\n                        \n                        print(f\"Rewards:\")\n                        print(f\"  Cumulative: {cumulative_reward:6.2f}\")\n                        print(f\"  Recent avg: {recent_reward:6.2f} (last 10 rounds)\")\n                        print()\n                except Exception:\n                    pass  # Metrics not available yet\n                    \n        except Exception as e:\n            print(f\"Progress: Unable to load status ({e})\")\n            print()\n        \n        # Instructions\n        print(\"-\"*70)\n        print(\"Press 'Stop' button or Ctrl+C to halt training\")\n        print(f\"Next update in 60 seconds...\")\n        \n        # Exit if all completed\n        if running == 0:\n            print()\n            print(\"=\"*70)\n            print(\"✅ All nodes completed successfully!\")\n            print(\"=\"*70)\n            break\n        \n        time.sleep(60)  # Update every minute\n\nexcept KeyboardInterrupt:\n    print(\"\\n\" + \"=\"*70)\n    print(\"⚠️  Training interrupted by user\")\n    print(\"=\"*70)\n    print(\"\\nTerminating all node processes...\")\n    \n    for i, p in enumerate(processes):\n        if p.poll() is None:\n            print(f\"  Stopping node_{i}... (PID: {p.pid})\")\n            p.terminate()\n    \n    time.sleep(5)\n    \n    # Force kill if still running\n    for i, p in enumerate(processes):\n        if p.poll() is None:\n            print(f\"  Force killing node_{i}... (PID: {p.pid})\")\n            p.kill()\n    \n    print(\"\\n✓ All processes terminated\")\n    print(\"\\n💾 Note: Training state is checkpointed.\")\n    print(\"   Re-run this notebook to resume from last checkpoint.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. Check Real-Time Progress from GDrive (Optional)\n",
    "\n",
    "**Reconnected after disconnect?** Run this cell to check training progress:\n",
    "- Shows current round for each node\n",
    "- Displays elapsed time and GPU memory\n",
    "- Works even if your notebook disconnected\n",
    "\n",
    "Progress is saved to GDrive every round, logs flush every 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Real-Time Progress Viewer ===\n",
    "# Run this cell anytime to check progress from GDrive\n",
    "# Useful if you reconnect after notebook disconnect\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/rl-swarm')\n",
    "\n",
    "from rgym_exp.utils.progress_tracker import get_experiment_progress\n",
    "\n",
    "progress = get_experiment_progress(GDRIVE_BASE_PATH, EXPERIMENT_NAME)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REAL-TIME PROGRESS FROM GDRIVE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Experiment: {progress.get('experiment')}\")\n",
    "print()\n",
    "\n",
    "for node_id, node_data in progress.get('nodes', {}).items():\n",
    "    if 'error' in node_data:\n",
    "        print(f\"  {node_id}: {node_data['error']}\")\n",
    "    else:\n",
    "        print(f\"  {node_id}:\")\n",
    "        print(f\"    Latest event: {node_data.get('latest_event')}\")\n",
    "        print(f\"    Current round: {node_data.get('latest_round')}\")\n",
    "        \n",
    "        elapsed_sec = node_data.get('elapsed_seconds', 0)\n",
    "        elapsed_hours = elapsed_sec / 3600\n",
    "        print(f\"    Elapsed time: {elapsed_hours:.1f} hours\")\n",
    "        print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Note: Progress updates every round. Logs flush every 30s to GDrive.\")\n",
    "print(\"You can access logs directly in Google Drive even while training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. View Results & Analysis\n",
    "\n",
    "**After training completes, run this cell to:**\n",
    "- Load all metrics\n",
    "- Calculate cumulative rewards per node\n",
    "- Compare to paper's results\n",
    "- Generate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgym_exp.utils.experiment_manager import get_experiment_metrics\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Results: {EXPERIMENT_NAME}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Config: I={NUM_TRAIN_SAMPLES}, J={NUM_TRANSPLANT_TREES}, G={NUM_GENERATIONS}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Nodes: {NUM_NODES}\")\n",
    "print()\n",
    "\n",
    "# Load metrics\n",
    "df = get_experiment_metrics(GDRIVE_BASE_PATH, EXPERIMENT_NAME)\n",
    "\n",
    "if not df.empty:\n",
    "    # Calculate cumulative reward per node\n",
    "    node_rewards = df.groupby('node_id')['my_reward'].sum().sort_values(ascending=False)\n",
    "    total_reward = node_rewards.sum()\n",
    "    \n",
    "    print(\"Cumulative Rewards by Node:\")\n",
    "    for node_id, reward in node_rewards.items():\n",
    "        print(f\"  {node_id:10s}: {reward:7.2f}\")\n",
    "    print(f\"  {'TOTAL':10s}: {total_reward:7.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # Compare to paper's results\n",
    "    print(\"Comparison to Paper (Qwen2.5-0.5B):\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    if NUM_TRANSPLANT_TREES == 0:\n",
    "        paper_reward = 562\n",
    "        config_name = \"Baseline (8/0)\"\n",
    "        paper_improvement = \"—\"\n",
    "    elif NUM_TRAIN_SAMPLES == 6 and NUM_TRANSPLANT_TREES == 2:\n",
    "        paper_reward = 854\n",
    "        config_name = \"Config 1 (6/2)\"\n",
    "        paper_improvement = \"+52%\"\n",
    "    elif NUM_TRAIN_SAMPLES == 4 and NUM_TRANSPLANT_TREES == 4:\n",
    "        paper_reward = 1093\n",
    "        config_name = \"Config 2 (4/4) **BEST**\"\n",
    "        paper_improvement = \"+94%\"\n",
    "    elif NUM_TRAIN_SAMPLES == 2 and NUM_TRANSPLANT_TREES == 6:\n",
    "        paper_reward = 946\n",
    "        config_name = \"Config 3 (2/6)\"\n",
    "        paper_improvement = \"+68%\"\n",
    "    else:\n",
    "        paper_reward = None\n",
    "        config_name = f\"Custom ({NUM_TRAIN_SAMPLES}/{NUM_TRANSPLANT_TREES})\"\n",
    "        paper_improvement = \"N/A\"\n",
    "    \n",
    "    print(f\"  Configuration: {config_name}\")\n",
    "    if paper_reward:\n",
    "        print(f\"  Paper (Qwen2.5):  {paper_reward:7.2f} ({paper_improvement})\")\n",
    "        print(f\"  Ours (GPT-2):     {total_reward:7.2f} (~{total_reward/paper_reward*100:.1f}% of paper)\")\n",
    "    else:\n",
    "        print(f\"  Ours (GPT-2):     {total_reward:7.2f}\")\n",
    "    print()\n",
    "    \n",
    "    # If we have baseline results, calculate improvement\n",
    "    baseline_path = GDRIVE_BASE_PATH + '/experiments/sapo_gpt2_baseline_8loc0ext'\n",
    "    try:\n",
    "        baseline_df = get_experiment_metrics(GDRIVE_BASE_PATH, 'sapo_gpt2_baseline_8loc0ext')\n",
    "        if not baseline_df.empty and NUM_TRANSPLANT_TREES > 0:\n",
    "            baseline_reward = baseline_df['my_reward'].sum()\n",
    "            improvement = ((total_reward - baseline_reward) / baseline_reward) * 100\n",
    "            print(f\"Improvement vs Our Baseline:\")\n",
    "            print(f\"  Baseline (GPT-2): {baseline_reward:7.2f}\")\n",
    "            print(f\"  This config:      {total_reward:7.2f}\")\n",
    "            print(f\"  Improvement:      {improvement:+7.1f}%\")\n",
    "            print()\n",
    "            \n",
    "            if improvement > 94:\n",
    "                print(\"✅ HYPOTHESIS CONFIRMED! GPT-2 shows >94% improvement\")\n",
    "                print(\"   Weaker models benefit MORE from swarm (as predicted)\")\n",
    "            elif improvement > 50:\n",
    "                print(\"✅ Strong swarm effect demonstrated (+{:.1f}%)\".format(improvement))\n",
    "            else:\n",
    "                print(\"⚠️  Lower improvement than expected\")\n",
    "            print()\n",
    "    except:\n",
    "        pass  # Baseline not run yet\n",
    "    \n",
    "    # Plot rewards over time\n",
    "    print(\"Generating plots...\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Cumulative reward per node\n",
    "    for node_id in df['node_id'].unique():\n",
    "        node_df = df[df['node_id'] == node_id].sort_values('round')\n",
    "        ax1.plot(node_df['round'], node_df['my_reward'].cumsum(), \n",
    "                 label=node_id, alpha=0.7)\n",
    "    \n",
    "    ax1.set_xlabel('Round', fontsize=12)\n",
    "    ax1.set_ylabel('Cumulative Reward', fontsize=12)\n",
    "    ax1.set_title(f'{config_name} - Cumulative Rewards Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Average reward per round (smoothed)\n",
    "    round_avg = df.groupby('round')['my_reward'].mean()\n",
    "    # Apply moving average\n",
    "    window_size = 100\n",
    "    smoothed = round_avg.rolling(window=window_size, center=True).mean()\n",
    "    \n",
    "    ax2.plot(round_avg.index, round_avg.values, alpha=0.3, color='gray', label='Raw')\n",
    "    ax2.plot(smoothed.index, smoothed.values, linewidth=2, color='blue', label=f'Smoothed ({window_size}-round MA)')\n",
    "    \n",
    "    ax2.set_xlabel('Round', fontsize=12)\n",
    "    ax2.set_ylabel('Average Reward', fontsize=12)\n",
    "    ax2.set_title(f'{config_name} - Average Reward per Round', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_path = f'/content/drive/MyDrive/rl-swarm/results_{EXPERIMENT_NAME}.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Plot saved to: {plot_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No metrics available yet\")\n",
    "    print(\"   Training may not have started or metrics file is missing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}